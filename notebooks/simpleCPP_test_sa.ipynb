{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simpleCPP_test_sa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This is a usage example of simpleCPP_test.py in pyhddmjags\n",
        "\n",
        "It follows a script in the [pyhddmjags](https://github.com/mdnunez/pyhddmjags) repository by Michael D. Nunez\n",
        "\n",
        "This script will run nicely on [Google Colab](https://colab.research.google.com/) without needing to install anything yourself.\n",
        "\n",
        "If you wish to install things locally, please avoid running the first part of this script and instead see [the installation instructions](https://github.com/mdnunez/pyhddmjags/blob/master/jags_wiener_ubuntu.md). Note that installing JAGS-WIENER on Windows with the latest version of JAGS is difficult.\n",
        "\n"
      ],
      "metadata": {
        "id": "hKn3NKOgp7-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first steps install JAGS, JAGS-WIENER, and pyjags directly from Python onto Ubuntu 18.04 hosted by Google.\n",
        "See the [installation steps](https://github.com/mdnunez/pyhddmjags/blob/master/jags_wiener_ubuntu.md) for your own Ubuntu, Mac, or Windows Linux Subsystem installation.\n",
        "\n"
      ],
      "metadata": {
        "id": "XyxTcb4wqTZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWaW0vUCmF-U"
      },
      "outputs": [],
      "source": [
        "!lsb_release -a\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install jags"
      ],
      "metadata": {
        "id": "Fakt_nu-mjCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which jags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjIMMXbYnEtr",
        "outputId": "a7cf84bd-af95-4d22-8f05-87ff957b7187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/jags\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"TEMP_DEB='$(mktemp)' \")\n",
        "os.system(\"wget -O '$TEMP_DEB' 'https://launchpad.net/~cidlab/+archive/ubuntu/jwm/+files/jags-wiener-module_1.1-5_amd64.deb'\")\n",
        "os.system(\"sudo dpkg -i '$TEMP_DEB'\")\n",
        "os.system(\"rm -f '$TEMP_DEB'\")"
      ],
      "metadata": {
        "id": "DzTVs4jcnmha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asZ9ShxepIRt",
        "outputId": "57d717c2-91a9-4abb-b5c5-059a28a25842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to JAGS 4.3.0 on Mon Aug  1 13:44:26 2022\n",
            "JAGS is free software and comes with ABSOLUTELY NO WARRANTY\n",
            "Loading module: basemod: ok\n",
            "Loading module: bugs: ok\n",
            ". load wiener\n",
            "Loading module: wiener: ok\n",
            ". load dic\n",
            "Loading module: dic: ok\n",
            ". exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyjags"
      ],
      "metadata": {
        "id": "RSLTS7bjpvw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAGS, JAGS-WIENER, and pyjags are now installed!\n",
        "\n",
        "# Now let's run [simpleCPP_test.py](https://github.com/mdnunez/pyhddmjags/blob/master/simpleCPP_test.py) from the [pyhddmjags](https://github.com/mdnunez/pyhddmjags) repository:\n",
        "\n",
        "First let's import the necessary modules"
      ],
      "metadata": {
        "id": "VHzLkydcqxm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import pyjags\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib import rc\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "X5viCRvbtFi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make some definitions that we need later."
      ],
      "metadata": {
        "id": "-EgzU6c61-Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate diffusion models quickly with intrinsic trial-to-trial variability in parameters\n",
        "def simulratcliff(N=100,Alpha=1,Tau=.4,Nu=1,Beta=.5,rangeTau=0,rangeBeta=0,Eta=.3,Varsigma=1):\n",
        "    \"\"\"\n",
        "    SIMULRATCLIFF  Generates data according to a drift diffusion model with optional trial-to-trial variability\n",
        "\n",
        "\n",
        "    Reference:\n",
        "    Tuerlinckx, F., Maris, E.,\n",
        "    Ratcliff, R., & De Boeck, P. (2001). A comparison of four methods for\n",
        "    simulating the diffusion process. Behavior Research Methods,\n",
        "    Instruments, & Computers, 33, 443-456.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N: a integer denoting the size of the output vector\n",
        "    (defaults to 100 experimental trials)\n",
        "\n",
        "    Alpha: the mean boundary separation across trials  in evidence units\n",
        "    (defaults to 1 evidence unit)\n",
        "\n",
        "    Tau: the mean non-decision time across trials in seconds\n",
        "    (defaults to .4 seconds)\n",
        "\n",
        "    Nu: the mean drift rate across trials in evidence units per second\n",
        "    (defaults to 1 evidence units per second, restricted to -5 to 5 units)\n",
        "\n",
        "    Beta: the initial bias in the evidence process for choice A as a proportion of boundary Alpha\n",
        "    (defaults to .5 or 50% of total evidence units given by Alpha)\n",
        "\n",
        "    rangeTau: Non-decision time across trials is generated from a uniform\n",
        "    distribution of Tau - rangeTau/2 to  Tau + rangeTau/2 across trials\n",
        "    (defaults to 0 seconds)\n",
        "\n",
        "    rangeZeta: Bias across trials is generated from a uniform distribution\n",
        "    of Zeta - rangeZeta/2 to Zeta + rangeZeta/2 across trials\n",
        "    (defaults to 0 evidence units)\n",
        "\n",
        "    Eta: Standard deviation of the drift rate across trials\n",
        "    (defaults to 3 evidence units per second, restricted to less than 3 evidence units)\n",
        "\n",
        "    Varsigma: The diffusion coefficient, the standard deviation of the\n",
        "    evidence accumulation process within one trial. It is recommended that\n",
        "    this parameter be kept fixed unless you have reason to explore this parameter\n",
        "    (defaults to 1 evidence unit per second)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy array with reaction times (in seconds) multiplied by the response vector\n",
        "    such that negative reaction times encode response B and positive reaction times\n",
        "    encode response A \n",
        "    \n",
        "    \n",
        "    Converted from simuldiff.m MATLAB script by Joachim Vandekerckhove\n",
        "    See also http://ppw.kuleuven.be/okp/dmatoolbox.\n",
        "    \"\"\"\n",
        "\n",
        "    if (Nu < -5) or (Nu > 5):\n",
        "        Nu = np.sign(Nu)*5\n",
        "        warnings.warn('Nu is not in the range [-5 5], bounding drift rate to %.1f...' % (Nu))\n",
        "\n",
        "    if (Eta > 3):\n",
        "        warning.warn('Standard deviation of drift rate is out of bounds, bounding drift rate to 3')\n",
        "        eta = 3\n",
        "\n",
        "    if (Eta == 0):\n",
        "        Eta = 1e-16\n",
        "\n",
        "    #Initialize output vectors\n",
        "    result = np.zeros(N)\n",
        "    T = np.zeros(N)\n",
        "    XX = np.zeros(N)\n",
        "\n",
        "    #Called sigma in 2001 paper\n",
        "    D = np.power(Varsigma,2)/2\n",
        "\n",
        "    #Program specifications\n",
        "    eps = 2.220446049250313e-16 #precision from 1.0 to next double-precision number\n",
        "    delta=eps\n",
        "\n",
        "    for n in range(0,N):\n",
        "        r1 = np.random.normal()\n",
        "        mu = Nu + r1*Eta\n",
        "        bb = Beta - rangeBeta/2 + rangeBeta*np.random.uniform()\n",
        "        zz = bb*Alpha\n",
        "        finish = 0\n",
        "        totaltime = 0\n",
        "        startpos = 0\n",
        "        Aupper = Alpha - zz\n",
        "        Alower = -zz\n",
        "        radius = np.min(np.array([np.abs(Aupper), np.abs(Alower)]))\n",
        "        while (finish==0):\n",
        "            lambda_ = 0.25*np.power(mu,2)/D + 0.25*D*np.power(np.pi,2)/np.power(radius,2)\n",
        "            # eq. formula (13) in 2001 paper with D = sigma^2/2 and radius = Alpha/2\n",
        "            F = D*np.pi/(radius*mu)\n",
        "            F = np.power(F,2)/(1 + np.power(F,2) )\n",
        "            # formula p447 in 2001 paper\n",
        "            prob = np.exp(radius*mu/D)\n",
        "            prob = prob/(1 + prob)\n",
        "            dir_ = 2*(np.random.uniform() < prob) - 1\n",
        "            l = -1\n",
        "            s2 = 0\n",
        "            while (s2>l):\n",
        "                s2=np.random.uniform()\n",
        "                s1=np.random.uniform()\n",
        "                tnew=0\n",
        "                told=0\n",
        "                uu=0\n",
        "                while (np.abs(tnew-told)>eps) or (uu==0):\n",
        "                    told=tnew\n",
        "                    uu=uu+1\n",
        "                    tnew = told + (2*uu+1) * np.power(-1,uu) * np.power(s1,(F*np.power(2*uu+1,2)));\n",
        "                    # infinite sum in formula (16) in BRMIC,2001\n",
        "                l = 1 + np.power(s1,(-F)) * tnew;\n",
        "            # rest of formula (16)\n",
        "            t = np.abs(np.log(s1))/lambda_;\n",
        "            # is the negative of t* in (14) in BRMIC,2001\n",
        "            totaltime=totaltime+t\n",
        "            dir_=startpos+dir_*radius\n",
        "            ndt = Tau - rangeTau/2 + rangeTau*np.random.uniform()\n",
        "            if ( (dir_ + delta) > Aupper):\n",
        "                T[n]=ndt+totaltime\n",
        "                XX[n]=1\n",
        "                finish=1\n",
        "            elif ( (dir_-delta) < Alower ):\n",
        "                T[n]=ndt+totaltime\n",
        "                XX[n]=-1\n",
        "                finish=1\n",
        "            else:\n",
        "                startpos=dir_\n",
        "                radius=np.min(np.abs([Aupper, Alower]-startpos))\n",
        "\n",
        "    result = T*XX\n",
        "    return result\n",
        "\n",
        "\n",
        "def diagnostic(insamples):\n",
        "    \"\"\"\n",
        "    Returns two versions of Rhat (measure of convergence, less is better with an approximate\n",
        "    1.10 cutoff) and Neff, number of effective samples). Note that 'rhat' is more diagnostic than 'oldrhat' according to \n",
        "    Gelman et al. (2014).\n",
        "\n",
        "    Reference for preferred Rhat calculation (split chains) and number of effective sample calculation: \n",
        "        Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. & Rubin, D. B. (2014). \n",
        "        Bayesian data analysis (Third Edition). CRC Press:\n",
        "        Boca Raton, FL\n",
        "\n",
        "    Reference for original Rhat calculation:\n",
        "        Gelman, A., Carlin, J., Stern, H., & Rubin D., (2004).\n",
        "        Bayesian Data Analysis (Second Edition). Chapman & Hall/CRC:\n",
        "        Boca Raton, FL.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    insamples: dic\n",
        "        Sampled values of monitored variables as a dictionary where keys\n",
        "        are variable names and values are numpy arrays with shape:\n",
        "        (dim_1, dim_n, iterations, chains). dim_1, ..., dim_n describe the\n",
        "        shape of variable in JAGS model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict:\n",
        "        rhat, oldrhat, neff, posterior mean, and posterior std for each variable. Prints maximum Rhat and minimum Neff across all variables\n",
        "    \"\"\"\n",
        "\n",
        "    result = {}  # Initialize dictionary\n",
        "    maxrhatsold = np.zeros((len(insamples.keys())), dtype=float)\n",
        "    maxrhatsnew = np.zeros((len(insamples.keys())), dtype=float)\n",
        "    minneff = np.ones((len(insamples.keys())), dtype=float)*np.inf\n",
        "    allkeys ={} # Initialize dictionary\n",
        "    keyindx = 0\n",
        "    for key in insamples.keys():\n",
        "        if key[0] != '_':\n",
        "            result[key] = {}\n",
        "            \n",
        "            possamps = insamples[key]\n",
        "            \n",
        "            # Number of chains\n",
        "            nchains = possamps.shape[-1]\n",
        "            \n",
        "            # Number of samples per chain\n",
        "            nsamps = possamps.shape[-2]\n",
        "            \n",
        "            # Number of variables per key\n",
        "            nvars = np.prod(possamps.shape[0:-2])\n",
        "            \n",
        "            # Reshape data\n",
        "            allsamps = np.reshape(possamps, possamps.shape[:-2] + (nchains * nsamps,))\n",
        "\n",
        "            # Reshape data to preduce R_hatnew\n",
        "            possampsnew = np.empty(possamps.shape[:-2] + (int(nsamps/2), nchains * 2,))\n",
        "            newc=0\n",
        "            for c in range(nchains):\n",
        "                possampsnew[...,newc] = np.take(np.take(possamps,np.arange(0,int(nsamps/2)),axis=-2),c,axis=-1)\n",
        "                possampsnew[...,newc+1] = np.take(np.take(possamps,np.arange(int(nsamps/2),nsamps),axis=-2),c,axis=-1)\n",
        "                newc += 2\n",
        "\n",
        "            # Index of variables\n",
        "            varindx = np.arange(nvars).reshape(possamps.shape[0:-2])\n",
        "            \n",
        "            # Reshape data\n",
        "            alldata = np.reshape(possamps, (nvars, nsamps, nchains))\n",
        "                    \n",
        "            # Mean of each chain for rhat\n",
        "            chainmeans = np.mean(possamps, axis=-2)\n",
        "            # Mean of each chain for rhatnew\n",
        "            chainmeansnew = np.mean(possampsnew, axis=-2)\n",
        "            # Global mean of each parameter for rhat\n",
        "            globalmean = np.mean(chainmeans, axis=-1)\n",
        "            globalmeannew = np.mean(chainmeansnew, axis=-1)\n",
        "            result[key]['mean'] = globalmean\n",
        "            result[key]['std'] = np.std(allsamps, axis=-1)\n",
        "            globalmeanext = np.expand_dims(\n",
        "                globalmean, axis=-1)  # Expand the last dimension\n",
        "            globalmeanext = np.repeat(\n",
        "                globalmeanext, nchains, axis=-1)  # For differencing\n",
        "            globalmeanextnew = np.expand_dims(\n",
        "                globalmeannew, axis=-1)  # Expand the last dimension\n",
        "            globalmeanextnew = np.repeat(\n",
        "                globalmeanextnew, nchains*2, axis=-1)  # For differencing\n",
        "            # Between-chain variance for rhat\n",
        "            between = np.sum(np.square(chainmeans - globalmeanext),\n",
        "                             axis=-1) * nsamps / (nchains - 1.)\n",
        "            # Mean of the variances of each chain for rhat\n",
        "            within = np.mean(np.var(possamps, axis=-2), axis=-1)\n",
        "            # Total estimated variance for rhat\n",
        "            totalestvar = (1. - (1. / nsamps)) * \\\n",
        "                within + (1. / nsamps) * between\n",
        "            # Rhat (original Gelman-Rubin statistic)\n",
        "            temprhat = np.sqrt(totalestvar / within)\n",
        "            maxrhatsold[keyindx] = np.nanmax(temprhat) # Ignore NANs\n",
        "            allkeys[keyindx] = key\n",
        "            result[key]['oldrhat'] = temprhat\n",
        "            # Between-chain variance for rhatnew\n",
        "            betweennew = np.sum(np.square(chainmeansnew - globalmeanextnew),\n",
        "                             axis=-1) * (nsamps/2) / ((nchains*2) - 1.)\n",
        "            # Mean of the variances of each chain for rhatnew\n",
        "            withinnew = np.mean(np.var(possampsnew, axis=-2), axis=-1)\n",
        "            # Total estimated variance\n",
        "            totalestvarnew = (1. - (1. / (nsamps/2))) * \\\n",
        "                withinnew + (1. / (nsamps/2)) * betweennew\n",
        "            # Rhatnew (Gelman-Rubin statistic from Gelman et al., 2013)\n",
        "            temprhatnew = np.sqrt(totalestvarnew / withinnew)\n",
        "            maxrhatsnew[keyindx] = np.nanmax(temprhatnew) # Ignore NANs\n",
        "            result[key]['rhat'] = temprhatnew\n",
        "            # Number of effective samples from Gelman et al. (2013) 286-288\n",
        "            neff = np.empty(possamps.shape[0:-2])\n",
        "            for v in range(0, nvars):\n",
        "                whereis = np.where(varindx == v)\n",
        "                rho_hat = []\n",
        "                rho_hat_even = 0\n",
        "                rho_hat_odd = 0\n",
        "                t = 2\n",
        "                while ((t < nsamps - 2) & (float(rho_hat_even) + float(rho_hat_odd) >= 0)):\n",
        "                    variogram_odd = np.mean(np.mean(np.power(alldata[v,(t-1):nsamps,:] - alldata[v,0:(nsamps-t+1),:],2),axis=0)) # above equation (11.7) in Gelman et al., 2013\n",
        "                    rho_hat_odd = 1 - np.divide(variogram_odd, 2*totalestvar[whereis]) # Equation (11.7) in Gelman et al., 2013\n",
        "                    rho_hat.append(rho_hat_odd)\n",
        "                    variogram_even = np.mean(np.mean(np.power(alldata[v,t:nsamps,:] - alldata[v,0:(nsamps-t),:],2),axis=0)) # above equation (11.7) in Gelman et al., 2013\n",
        "                    rho_hat_even = 1 - np.divide(variogram_even, 2*totalestvar[whereis]) # Equation (11.7) in Gelman et al., 2013\n",
        "                    rho_hat.append(rho_hat_even)\n",
        "                    t += 2\n",
        "                rho_hat = np.asarray(rho_hat)\n",
        "                neff[whereis] = np.divide(nchains*nsamps, 1 + 2*np.sum(rho_hat)) # Equation (11.8) in Gelman et al., 2013\n",
        "            result[key]['neff'] = np.round(neff) \n",
        "            minneff[keyindx] = np.nanmin(np.round(neff))\n",
        "            keyindx += 1\n",
        "\n",
        "            # Geweke statistic?\n",
        "    # print(\"Maximum old Rhat was %3.2f for variable %s\" % (np.max(maxrhatsold),allkeys[np.argmax(maxrhatsold)]))\n",
        "    maxrhatkey = allkeys[np.argmax(maxrhatsnew)]\n",
        "    maxrhatindx = np.unravel_index( np.argmax(result[maxrhatkey]['rhat']) , result[maxrhatkey]['rhat'].shape)\n",
        "    print(\"Maximum Rhat was %3.2f for variable %s at index %s\" % (np.max(maxrhatsnew), maxrhatkey, maxrhatindx))\n",
        "    minneffkey = allkeys[np.argmin(minneff)]\n",
        "    minneffindx = np.unravel_index( np.argmin(result[minneffkey]['neff']) , result[minneffkey]['neff'].shape)\n",
        "    print(\"Minimum number of effective samples was %d for variable %s at index %s\" % (np.min(minneff), minneffkey, minneffindx))\n",
        "    return result\n",
        "\n",
        "\n",
        "def recovery(possamps, truevals):  # Parameter recovery plots\n",
        "    \"\"\"Plots true parameters versus 99% and 95% credible intervals of recovered\n",
        "    parameters. Also plotted are the median (circles) and mean (stars) of the posterior\n",
        "    distributions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    possamps : ndarray of posterior chains where the last dimension is the\n",
        "    number of chains, the second to last dimension is the number of samples in\n",
        "    each chain, all other dimensions must match the dimensions of truevals\n",
        "\n",
        "    truevals : ndarray of true parameter values\n",
        "    \"\"\"\n",
        "\n",
        "    # Number of chains\n",
        "    nchains = possamps.shape[-1]\n",
        "\n",
        "    # Number of samples per chain\n",
        "    nsamps = possamps.shape[-2]\n",
        "\n",
        "    # Number of variables to plot\n",
        "    nvars = np.prod(possamps.shape[0:-2])\n",
        "\n",
        "    # Reshape data\n",
        "    alldata = np.reshape(possamps, (nvars, nchains, nsamps))\n",
        "    alldata = np.reshape(alldata, (nvars, nchains * nsamps))\n",
        "    truevals = np.reshape(truevals, (nvars))\n",
        "\n",
        "    # Plot properties\n",
        "    LineWidths = np.array([2, 5])\n",
        "    teal = np.array([0, .7, .7])\n",
        "    blue = np.array([0, 0, 1])\n",
        "    orange = np.array([1, .3, 0])\n",
        "    Colors = [teal, blue]\n",
        "\n",
        "    for v in range(0, nvars):\n",
        "        # Compute percentiles\n",
        "        bounds = stats.scoreatpercentile(alldata[v, :], (.5, 2.5, 97.5, 99.5))\n",
        "        for b in range(0, 2):\n",
        "            # Plot credible intervals\n",
        "            credint = np.ones(100) * truevals[v]\n",
        "            y = np.linspace(bounds[b], bounds[-1 - b], 100)\n",
        "            lines = plt.plot(credint, y)\n",
        "            plt.setp(lines, color=Colors[b], linewidth=LineWidths[b])\n",
        "            if b == 1:\n",
        "                # Mark median\n",
        "                mmedian = plt.plot(truevals[v], np.median(alldata[v, :]), 'o')\n",
        "                plt.setp(mmedian, markersize=10, color=[0., 0., 0.])\n",
        "                # Mark mean\n",
        "                mmean = plt.plot(truevals[v], np.mean(alldata[v, :]), '*')\n",
        "                plt.setp(mmean, markersize=10, color=teal)\n",
        "    # Plot line y = x\n",
        "    tempx = np.linspace(np.min(truevals), np.max(\n",
        "        truevals), num=100)\n",
        "    recoverline = plt.plot(tempx, tempx)\n",
        "    plt.setp(recoverline, linewidth=3, color=orange)\n",
        "\n",
        "def jellyfish(possamps):  # jellyfish plots\n",
        "    \"\"\"Plots posterior distributions of given posterior samples in a jellyfish\n",
        "    plot. Jellyfish plots are posterior distributions (mirrored over their\n",
        "    horizontal axes) with 99% and 95% credible intervals (currently plotted\n",
        "    from the .5% and 99.5% & 2.5% and 97.5% percentiles respectively.\n",
        "    Also plotted are the median and mean of the posterior distributions\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    possamps : ndarray of posterior chains where the last dimension is\n",
        "    the number of chains, the second to last dimension is the number of samples\n",
        "    in each chain, all other dimensions describe the shape of the parameter\n",
        "    \"\"\"\n",
        "\n",
        "    # Number of chains\n",
        "    nchains = possamps.shape[-1]\n",
        "\n",
        "    # Number of samples per chain\n",
        "    nsamps = possamps.shape[-2]\n",
        "\n",
        "    # Number of dimensions\n",
        "    ndims = possamps.ndim - 2\n",
        "\n",
        "    # Number of variables to plot\n",
        "    nvars = np.prod(possamps.shape[0:-2])\n",
        "\n",
        "    # Index of variables\n",
        "    varindx = np.arange(nvars).reshape(possamps.shape[0:-2])\n",
        "\n",
        "    # Reshape data\n",
        "    alldata = np.reshape(possamps, (nvars, nchains, nsamps))\n",
        "    alldata = np.reshape(alldata, (nvars, nchains * nsamps))\n",
        "\n",
        "    # Plot properties\n",
        "    LineWidths = np.array([2, 5])\n",
        "    teal = np.array([0, .7, .7])\n",
        "    blue = np.array([0, 0, 1])\n",
        "    orange = np.array([1, .3, 0])\n",
        "    Colors = [teal, blue]\n",
        "\n",
        "    # Initialize ylabels list\n",
        "    ylabels = ['']\n",
        "\n",
        "    for v in range(0, nvars):\n",
        "        # Create ylabel\n",
        "        whereis = np.where(varindx == v)\n",
        "        newlabel = ''\n",
        "        for l in range(0, ndims):\n",
        "            newlabel = newlabel + ('_%i' % whereis[l][0])\n",
        "\n",
        "        ylabels.append(newlabel)\n",
        "\n",
        "        # Compute posterior density curves\n",
        "        kde = stats.gaussian_kde(alldata[v, :])\n",
        "        bounds = stats.scoreatpercentile(alldata[v, :], (.5, 2.5, 97.5, 99.5))\n",
        "        for b in range(0, 2):\n",
        "            # Bound by .5th percentile and 99.5th percentile\n",
        "            x = np.linspace(bounds[b], bounds[-1 - b], 100)\n",
        "            p = kde(x)\n",
        "\n",
        "            # Scale distributions down\n",
        "            maxp = np.max(p)\n",
        "\n",
        "            # Plot jellyfish\n",
        "            upper = .25 * p / maxp + v + 1\n",
        "            lower = -.25 * p / maxp + v + 1\n",
        "            lines = plt.plot(x, upper, x, lower)\n",
        "            plt.setp(lines, color=Colors[b], linewidth=LineWidths[b])\n",
        "            if b == 1:\n",
        "                # Mark mode\n",
        "                wheremaxp = np.argmax(p)\n",
        "                mmode = plt.plot(np.array([1., 1.]) * x[wheremaxp],\n",
        "                                 np.array([lower[wheremaxp], upper[wheremaxp]]))\n",
        "                plt.setp(mmode, linewidth=3, color=orange)\n",
        "                # Mark median\n",
        "                mmedian = plt.plot(np.median(alldata[v, :]), v + 1, 'ko')\n",
        "                plt.setp(mmedian, markersize=10, color=[0., 0., 0.])\n",
        "                # Mark mean\n",
        "                mmean = plt.plot(np.mean(alldata[v, :]), v + 1, '*')\n",
        "                plt.setp(mmean, markersize=10, color=teal)\n",
        "\n",
        "    # Display plot\n",
        "    plt.setp(plt.gca(), yticklabels=ylabels, yticks=np.arange(0, nvars + 1))\n",
        "\n"
      ],
      "metadata": {
        "id": "dAIDDhLs2HED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some folders to save some data and figures"
      ],
      "metadata": {
        "id": "eWgN1LE65AY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.system('mkdir data')\n",
        "os.system('mkdir jagscode')\n",
        "os.system('mkdir modelfits')\n",
        "os.system('mkdir figures')"
      ],
      "metadata": {
        "id": "F9E_s-AE1rug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run the simulation of the model."
      ],
      "metadata": {
        "id": "zEEQHVBeviIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Simulations ###\n",
        "\n",
        "# Generate samples from the joint-model of reaction time and choice\n",
        "# Note you could remove this if statement and replace with loading your own data to dictionary \"gendata\"\n",
        "\n",
        "if not os.path.exists('data/simpleEEG_test1.mat'):\n",
        "    # Number of simulated participants\n",
        "    nparts = 100\n",
        "\n",
        "    # Number of trials for one participant\n",
        "    ntrials = 100\n",
        "\n",
        "    # Number of total trials in each simulation\n",
        "    N = ntrials * nparts\n",
        "\n",
        "    # Set random seed\n",
        "    np.random.seed(2021)\n",
        "\n",
        "    ndt = np.random.uniform(.15, .6, size=nparts)  # Uniform from .15 to .6 seconds\n",
        "    alpha = np.random.uniform(.8, 1.4, size=nparts)  # Uniform from .8 to 1.4 evidence units\n",
        "    beta = np.random.uniform(.3, .7, size=nparts)  # Uniform from .3 to .7 * alpha\n",
        "    delta = np.random.uniform(-4, 4, size=nparts)  # Uniform from -4 to 4 evidence units per second\n",
        "    deltatrialsd = np.random.uniform(0, 2, size=nparts)  # Uniform from 0 to 2 evidence units per second\n",
        "    CPPnoise = np.random.uniform(0, 1, size=nparts) # Uniform from 0 to 1 evidence units per second\n",
        "    y = np.zeros(N)\n",
        "    rt = np.zeros(N)\n",
        "    acc = np.zeros(N)\n",
        "    CPP = np.zeros(N)\n",
        "    participant = np.zeros(N)  # Participant index\n",
        "    indextrack = np.arange(ntrials)\n",
        "    for p in range(nparts):\n",
        "        tempout = simulratcliff(N=ntrials, Alpha=alpha[p], Tau=ndt[p], Beta=beta[p],\n",
        "                                     Nu=delta[p], Eta=deltatrialsd[p])\n",
        "        tempx = np.sign(np.real(tempout))\n",
        "        tempt = np.abs(np.real(tempout))\n",
        "        CPP[indextrack] = np.random.normal(loc=delta[p],scale=CPPnoise[p],size=ntrials)\n",
        "        y[indextrack] = tempx * tempt\n",
        "        rt[indextrack] = tempt\n",
        "        acc[indextrack] = (tempx + 1) / 2\n",
        "        participant[indextrack] = p + 1\n",
        "        indextrack += ntrials\n",
        "\n",
        "    genparam = dict()\n",
        "    genparam['ndt'] = ndt\n",
        "    genparam['beta'] = beta\n",
        "    genparam['alpha'] = alpha\n",
        "    genparam['delta'] = delta\n",
        "    genparam['deltatrialsd'] = deltatrialsd\n",
        "    genparam['CPPnoise'] = CPPnoise\n",
        "    genparam['CPP'] = CPP\n",
        "    genparam['rt'] = rt\n",
        "    genparam['acc'] = acc\n",
        "    genparam['y'] = y\n",
        "    genparam['participant'] = participant\n",
        "    genparam['nparts'] = nparts\n",
        "    genparam['ntrials'] = ntrials\n",
        "    genparam['N'] = N\n",
        "    sio.savemat('data/simpleEEG_test1.mat', genparam)\n",
        "else:\n",
        "    genparam = sio.loadmat('data/simpleEEG_test1.mat')\n"
      ],
      "metadata": {
        "id": "-oa1nOf613ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's fit the model with pyjags!"
      ],
      "metadata": {
        "id": "WTu8ZF983EQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAGS code\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(2020)\n",
        "\n",
        "tojags = '''\n",
        "model {\n",
        "    \n",
        "    ##########\n",
        "    #Simple NDDM parameter priors\n",
        "    ##########\n",
        "    for (p in 1:nparts) {\n",
        "    \n",
        "        #Boundary parameter (speed-accuracy tradeoff) per participant\n",
        "        alpha[p] ~ dnorm(1, pow(.5,-2))T(0, 3)\n",
        "\n",
        "        #Non-decision time per participant\n",
        "        ndt[p] ~ dnorm(.5, pow(.25,-2))T(0, 1)\n",
        "\n",
        "        #Start point bias towards choice A per participant\n",
        "        beta[p] ~ dnorm(.5, pow(.25,-2))T(0, 1)\n",
        "\n",
        "        #Drift rate to choice A per participant\n",
        "        delta[p] ~ dnorm(0, pow(2, -2))\n",
        "\n",
        "        #Noise in observed EEG measure, the CentroParietal Positivity (CPP) slope per participant\n",
        "        CPPnoise[p] ~ dnorm(1, pow(.5,-2))T(0, 3)\n",
        "\n",
        "    }\n",
        "\n",
        "    ##########\n",
        "    # Wiener likelihood\n",
        "    for (i in 1:N) {\n",
        "\n",
        "        # Observations of accuracy*RT for DDM process of rightward/leftward RT\n",
        "        y[i] ~ dwiener(alpha[participant[i]], ndt[participant[i]], beta[participant[i]], delta[participant[i]])\n",
        "\n",
        "        # Observations of CentroParietal Positivity (CPP) slope per trial\n",
        "        CPP[i] ~ dnorm(delta[participant[i]],pow(CPPnoise[participant[i]],-2))\n",
        "\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "# pyjags code\n",
        "\n",
        "# Make sure $LD_LIBRARY_PATH sees /usr/local/lib\n",
        "# Make sure that the correct JAGS/modules-4/ folder contains wiener.so and wiener.la\n",
        "pyjags.modules.load_module('wiener')\n",
        "pyjags.modules.load_module('dic')\n",
        "pyjags.modules.list_modules()\n",
        "\n",
        "nchains = 3\n",
        "burnin = 500  # Note that scientific notation breaks pyjags\n",
        "nsamps = 1000\n",
        "\n",
        "modelfile = 'jagscode/simpleCPP_test1.jags'\n",
        "f = open(modelfile, 'w')\n",
        "f.write(tojags)\n",
        "f.close()\n",
        "\n",
        "# Track these variables\n",
        "trackvars = ['alpha', 'ndt', 'beta', 'delta', 'CPPnoise']\n",
        "\n",
        "N = np.squeeze(genparam['N'])\n",
        "\n",
        "# Fit model to data\n",
        "y = np.squeeze(genparam['y'])\n",
        "rt = np.squeeze(genparam['rt'])\n",
        "CPP = np.squeeze(genparam['CPP'])\n",
        "participant = np.squeeze(genparam['participant'])\n",
        "nparts = np.squeeze(genparam['nparts'])\n",
        "ntrials = np.squeeze(genparam['ntrials'])\n",
        "\n",
        "minrt = np.zeros(nparts)\n",
        "for p in range(0, nparts):\n",
        "    minrt[p] = np.min(rt[(participant == (p + 1))])\n",
        "\n",
        "initials = []\n",
        "for c in range(0, nchains):\n",
        "    chaininit = {\n",
        "        'alpha': np.random.uniform(.5, 2., size=nparts),\n",
        "        'ndt': np.random.uniform(.1, .5, size=nparts),\n",
        "        'beta': np.random.uniform(.2, .8, size=nparts),\n",
        "        'delta': np.random.uniform(-4., 4., size=nparts),\n",
        "        'CPPnoise': np.random.uniform(.5, 2., size=nparts)\n",
        "    }\n",
        "    for p in range(0, nparts):\n",
        "        chaininit['ndt'][p] = np.random.uniform(0., minrt[p] / 2)\n",
        "    initials.append(chaininit)\n",
        "print('Fitting ''simpleEEG'' model ...')\n",
        "threaded = pyjags.Model(file=modelfile, init=initials,\n",
        "                        data=dict(y=y, CPP=CPP, N=N, nparts=nparts,\n",
        "                                  participant=participant),\n",
        "                        chains=nchains, adapt=burnin, threads=6,\n",
        "                        progress_bar=True)\n",
        "samples = threaded.sample(nsamps, vars=trackvars, thin=10)\n",
        "savestring = ('modelfits/simpleEEG_test1_simpleCPP.mat')\n",
        "print('Saving results to: \\n %s' % savestring)\n",
        "sio.savemat(savestring, samples)"
      ],
      "metadata": {
        "id": "_iNptp093IQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's check the diagnostics!\n",
        "\n"
      ],
      "metadata": {
        "id": "uWGulxVA5u7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostics\n",
        "samples = sio.loadmat(savestring)\n",
        "diags = diagnostic(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPgribgu3IUO",
        "outputId": "253188e7-619e-4072-bf61-8cf717cb1a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Rhat was 1.05 for variable beta at index (67,)\n",
            "Minimum number of effective samples was 34 for variable ndt at index (24,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And look at some posterior distributions!"
      ],
      "metadata": {
        "id": "6-sTrtF576jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Posterior distributions\n",
        "plt.figure()\n",
        "jellyfish(samples['alpha'])\n",
        "plt.title('Posterior distributions of boundary parameter')\n",
        "plt.savefig('figures/alpha_posteriors_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "jellyfish(samples['ndt'])\n",
        "plt.title('Posterior distributions of the non-decision time parameter')\n",
        "plt.savefig('figures/ndt_posteriors_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "jellyfish(samples['beta'])\n",
        "plt.title('Posterior distributions of the start point parameter')\n",
        "plt.savefig('figures/beta_posteriors_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "jellyfish(samples['delta'])\n",
        "plt.title('Posterior distributions of the drift-rate')\n",
        "plt.savefig('figures/delta_posteriors_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "jellyfish(samples['CPPnoise'])\n",
        "plt.title('Posterior distributions of the noise in the observed CPP slope')\n",
        "plt.savefig('figures/CPPnoise_posteriors_simpleCPP.png', format='png', bbox_inches=\"tight\")\n"
      ],
      "metadata": {
        "id": "iifkJDTq9Hi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And look at some recovery plots!"
      ],
      "metadata": {
        "id": "3d2FnGdU9d_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Recovery\n",
        "plt.figure()\n",
        "recovery(samples['alpha'], genparam['alpha'])\n",
        "plt.title('Recovery of boundary parameter')\n",
        "plt.savefig('figures/alpha_recovery_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "recovery(samples['ndt'], genparam['ndt'])\n",
        "plt.title('Recovery of the non-decision time parameter')\n",
        "plt.savefig('figures/ndt_recovery_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "recovery(samples['beta'], genparam['beta'])\n",
        "plt.title('Recovery of the start point parameter')\n",
        "plt.savefig('figures/beta_recovery_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "recovery(samples['delta'], genparam['delta'])\n",
        "plt.title('Recovery of the drift-rate')\n",
        "plt.savefig('figures/delta_recovery_simpleCPP.png', format='png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "recovery(samples['CPPnoise'], genparam['CPPnoise'])\n",
        "plt.title('Recovery of the noise in the observed CPP slope')\n",
        "plt.savefig('figures/CPPnoise_recovery_simpleCPP.png', format='png', bbox_inches=\"tight\")\n"
      ],
      "metadata": {
        "id": "2IJljD4R9k9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}